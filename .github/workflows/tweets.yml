name: Scrape Tweets to Supabase

on:
  # run every 3 hours (UTC)
  schedule:
    - cron:  '0 */3 * * *'
  # allow manual trigger from the Actions tab
  workflow_dispatch:

jobs:
  scrape:
    runs-on: ubuntu-latest

    steps:
      # 1️⃣  Checkout repository
      - uses: actions/checkout@v4

      # 2️⃣  Install R -------------------------------------------------
      - uses: r-lib/actions/setup-r@v2
        with:
          r-version: '4.3'

      # 3️⃣  System libraries for R packages that need compilation
      - name: Install system deps
        run: |
          sudo apt-get update
          sudo apt-get install -y --no-install-recommends \
            build-essential       \
            libpq-dev             \
            libssl-dev libcurl4-openssl-dev libxml2-dev \
            python3-dev python3-venv \
            libpng-dev zlib1g-dev

      # 4️⃣  Python for reticulate / twscrape --------------------------
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      # 5️⃣  Run the R script -----------------------------------------
      - name: Run fetch_twitter_to_supabase.R
        env:
          # ---- Supabase --------------------------------------------
          SUPABASE_HOST:   ${{ secrets.SUPABASE_HOST }}
          SUPABASE_PORT:   ${{ secrets.SUPABASE_PORT }}
          SUPABASE_DB:     ${{ secrets.SUPABASE_DB }}
          SUPABASE_USER:   ${{ secrets.SUPABASE_USER }}
          SUPABASE_PWD:    ${{ secrets.SUPABASE_PWD }}
          # ---- Twitter cookies -------------------------------------
          TW_COOKIES_JSON: ${{ secrets.TW_COOKIES_JSON }}
          # optional: comma-separated list of handles
          TW_HANDLES:      ${{ vars.TW_HANDLES }}
          # where the Python virtual-env will live on the runner
          PY_VENV_PATH:    ".venv"
        run: |
          Rscript fetch_twitter_to_supabase.R

